---
layout: post
title: Pyspark에서 AWS S3 데이터 읽기 (2025)
date: 2025-06-18 10:00:00 pm
permalink: posts/125
description: Pyspark에서 AWS S3 데이터 읽는 방법을 간단히 정리한다.
categories: [ETL]
tags: [spark, s3]
---

> Pyspark에서 AWS S3 데이터를 읽는 방법을 간단히 정리한다.

Spark 3.5.3 버전을 기준으로 작성되었다.

|  용도    |  이미지    |
|jupyter | quay.io/jupyter/pyspark-notebook:spark-3.5.3|
|sparkApplication | apache/spark:3.5.3-scala2.12-java17-python3-ubuntu|

S3AFileSystem은 Apache Hadoop 에코시스템에서 AWS S3와 연동하기 위한 파일 시스템 구현체이다.

## AWS 외부에서 활용

provider에는 **SimpleAWSCredentialsProvider**를 사용한다.

hadoop-aws:3.3.4가 해당 이미지에서 사용 가능한 최신 버전이다.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("AWS_S3_APP") \
    .config("spark.jars.packages",
        "org.apache.hadoop:hadoop-aws:3.3.4,"
    ) \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.hadoop.fs.s3a.access.key", "...") \
    .config("spark.hadoop.fs.s3a.secret.key", "...") \
    .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com") \
    .config("spark.hadoop.fs.s3a.endpoint.region", "ap-northeast-2") \
    .config("spark.hadoop.com.amazonaws.services.s3.enableV4", "true") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

FILE_S3_PATH = "s3a://[버킷]/[KEY].csv"

# CSV 파일을 DataFrame으로 읽기
df = spark.read.format("csv") \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .load(FILE_S3_PATH)
```

spark.jars.packages를 사용하면 의존성이 포함된 jar 파일이 자동으로 다운로드된다.

.ivy2/jars를 확인해보면 aws-java-sdk-bundle도 다운로드되어 있다.

S3에 접근할 때 hadoop-aws와 aws-java-sdk-bundle이 함께 사용된다.

![pyspark_s3_1]({{site.baseurl}}/assets/img/etl/pyspark_s3_1.png)

## MinIO 활용

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MinIOAPP") \
    .config("spark.jars.packages",
        "org.apache.hadoop:hadoop-aws:3.3.4,"
    ) \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.hadoop.fs.s3a.access.key", "...") \
    .config("spark.hadoop.fs.s3a.secret.key", "...") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio.common.svc:80") \ # MinIO endpoint
    .config("spark.hadoop.fs.s3a.endpoint.region", "ap-northeast-2") \ # MinIO region
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \ # http 일 경우,
    .getOrCreate()
```

## EC2에서 S3 접근

보안을 위해 provider에는 **TemporaryAWSCredentialsProvider**를 사용한다.

boto3를 통해 access key, secret key, session token을 가져와야 한다.

단, EC2 IAM role에 S3 접근 권한이 등록되어 있어야 한다.(예: S3FullAccess)

```python
import boto3
from pyspark.sql import SparkSession

sess = boto3.Session(region_name='ap-northeast-2')

credentials = sess.get_credentials()
frozen_credentials = credentials.get_frozen_credentials() # 자동 갱신하지 않는 credentials

spark = SparkSession.builder.appName("EC2_S3_APP") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider") \
    .config("spark.hadoop.fs.s3a.endpoint", "s3.ap-northeast-2.amazonaws.com") \
    .config("spark.hadoop.com.amazonaws.services.s3.enableV4", "true") \
    .config("spark.hadoop.fs.s3a.access.key", frozen_credentials.access_key) \
    .config("spark.hadoop.fs.s3a.secret.key", frozen_credentials.secret_key) \
    .config("spark.hadoop.fs.s3a.session.token", frozen_credentials.token)
    .getOrCreate()
```

## Dockerfile for SparkApplication

hadoop-aws와 aws-java-sdk-bundle을 추가한 형태로 직접 빌드하여 SparkApplication 배포에 사용할 수 있다.

/opt/spark/jars 폴더에 추가하여 설정없이 자동으로 로드된다. python 패키지는 requirements.txt에 추가하면 된다.

추가적인 jar 파일과 호환성을 유지하기 위해 scala 버전이 지정된 이미지를 사용하였다.

```python
FROM apache/spark:3.5.3-scala2.12-java17-python3-ubuntu

RUN curl -o /opt/spark/jars/hadoop-aws-3.3.4.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
    curl -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar 

USER root
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

WORKDIR /opt/spark/work-dir

ENTRYPOINT ["/opt/entrypoint.sh"]
```

requirements.txt

```python
boto3==1.38.37
```

참고: ENTRYPOINT ["/opt/entrypoint.sh"] 이 부분을 사용하지 않으면 SparkApplication 실행이 오래 걸리고 정상으로 종료 처리되지 않았다.

`References` : 

* [Authenticating with S3 - hadoop-aws](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/authentication.md){:target="_blank"}
